{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barankutluay/deep-learning-projects/blob/main/Fruits360_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcUYmrU6QN3q",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b004196-04dc-4199-c646-cdfb002ae053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: TensorFlow version: 2.12.0\n",
            "INFO: TensorFlow will be updated.\n"
          ]
        }
      ],
      "source": [
        "# @title # Upgrade Tensorflow\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import pytz\n",
        "import datetime\n",
        "import numpy as np\n",
        "import json\n",
        "import concurrent.futures\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "print(\"INFO: TensorFlow version:\", tf.__version__)\n",
        "\n",
        "if tf.__version__ != \"2.13.0\":\n",
        "    print(\"INFO: TensorFlow will be updated.\")\n",
        "\n",
        "    os.system(\"pip install tensorflow --upgrade\")\n",
        "\n",
        "    # Define the reason for restarting the runtime\n",
        "    restart_reason = \"Updating dependencies and configurations\"\n",
        "\n",
        "    # Display a warning message\n",
        "    warning_message = f\"WARNING: Runtime will restart because: {restart_reason}\"\n",
        "    print(warning_message)\n",
        "\n",
        "    os.kill(os.getpid(), 9)\n",
        "else:\n",
        "    print(\"INFO: TensorFlow has been updated before.\")\n",
        "    # Check if keras_cv is already installed\n",
        "    try:\n",
        "        import pkg_resources\n",
        "        pkg_resources.get_distribution(\"keras-cv\") and pkg_resources.get_distribution(\"python-dotenv\")\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        # If not installed, install it\n",
        "        os.system(\"pip install keras-cv python-dotenv\")\n",
        "\n",
        "    import keras_cv\n",
        "    from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN1N8fCb9Y_H",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title # Configuration { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "ACCELERATOR = \"TPU\" # @param [\"TPU\", \"GPU\", \"None\"]\n",
        "NOTEBOOK = \"Colab\" # @param [\"Colab\", \"Kaggle\"]\n",
        "GPU_MEMORY_LIMIT_GB = 10 # @param {type:\"slider\", min:2, max:24, step:2}\n",
        "\n",
        "ACCELERATOR = None if ACCELERATOR == \"None\" else ACCELERATOR\n",
        "TPU, TPU_STRATEGY = None, None\n",
        "\n",
        "def configure_notebook(accelerator=None):\n",
        "    global TPU, TPU_STRATEGY\n",
        "    if accelerator == \"TPU\":\n",
        "        # Connect to TPU\n",
        "        TPU = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "        TPU_STRATEGY = tf.distribute.TPUStrategy(TPU)\n",
        "        TPU_ADDRESS = f\"grpc://{TPU.cluster_spec().as_dict()['worker'][0]}\"\n",
        "        devices = tf.config.list_logical_devices(\"TPU\")\n",
        "        # tf.config.set_soft_device_placement(True)\n",
        "        print(\"INFO: TPU has been connected. TPU Address:\", TPU_ADDRESS)\n",
        "        print(\"\\nTPU Devices:\")\n",
        "        for device in devices:\n",
        "            print(f\"{device.name[-12:-5]} {device.name[-5:-2]} {device.name[-1:]}\")\n",
        "\n",
        "    elif accelerator == \"GPU\":\n",
        "        print(\"All Devices:\")\n",
        "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "        if gpus:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, False)\n",
        "                tf.config.experimental.set_virtual_device_configuration(\n",
        "                    gpu,\n",
        "                    [tf.config.experimental.VirtualDeviceConfiguration(\n",
        "                        memory_limit=GPU_MEMORY_LIMIT_GB * 1024\n",
        "                    )]\n",
        "                )\n",
        "            tf.config.set_visible_devices(gpus[0], \"GPU\")\n",
        "            logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
        "            print(f\"{len(gpus)} Physical GPU/GPUs, {len(logical_gpus)} Logical GPU/GPUs\")\n",
        "\n",
        "    else:\n",
        "        cpus = tf.config.list_physical_devices(\"CPU\")\n",
        "        logical_cpus = tf.config.list_logical_devices(\"CPU\")\n",
        "        print(f\"{len(cpus)} Physical CPU/CPUs, {len(logical_cpus)} Logical CPU/CPUs\")\n",
        "\n",
        "\n",
        "configure_notebook(accelerator=ACCELERATOR)\n",
        "\n",
        "print(\"\\nINFO: The configuration process is complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWIA2ETY9Y_M"
      },
      "outputs": [],
      "source": [
        "# @title # Download the Dataset { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "# @markdown # Configuration:\n",
        "DATA_SOURCE = \"Kaggle\" # @param [\"Kaggle\", \"Other\"]\n",
        "PROJECT_NAME = \"Fruits360_VGG16\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ## Directories:\n",
        "# @markdown Root directory:\n",
        "DATASET_DIR = \"fruits-360_dataset/fruits-360\" # @param {type:\"string\"}\n",
        "# @markdown Directory that contains training files:\n",
        "TRAIN_DIR = \"Training\" # @param {type:\"string\"}\n",
        "# @markdown Directory that contains testing files:\n",
        "TEST_DIR = \"Test\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ## Image Dimensions:\n",
        "IMG_HEIGHT = 100 # @param {type:\"integer\"}\n",
        "IMG_WIDTH = 100 # @param {type:\"integer\"}\n",
        "CHANNELS = 3 # @param [\"1\", \"3\", \"4\"] {type:\"raw\"}\n",
        "\n",
        "# @markdown ## Notes:\n",
        "# @markdown If data source is Kaggle, upload .env file that\n",
        "# @markdown contains your credentials to the Google Drive. Then you can specify\n",
        "# @markdown username and dataset name. \\\\\n",
        "# @markdown Usage: https://www.kaggle.com/datasets/{username}/{dataset_name}\n",
        "\n",
        "# @markdown ### If data source is Kaggle:\n",
        "if DATA_SOURCE == \"Kaggle\":\n",
        "    USERNAME = \"moltean\" # @param {type:\"string\"}\n",
        "    DATASET_NAME = \"fruits\" # @param {type:\"string\"}\n",
        "    ENV_PATH = \"/content/drive/MyDrive/.env\"\n",
        "\n",
        "# Define paths based on the notebook environment\n",
        "KAGGLE_INPUT_PATH = \"/kaggle/input/\" + DATASET_NAME\n",
        "KAGGLE_WORKING_PATH = \"/kaggle/working/\" + PROJECT_NAME\n",
        "COLAB_WORKING_PATH = os.path.abspath(PROJECT_NAME)\n",
        "DRIVE_PATH = os.path.abspath(\"drive\")\n",
        "\n",
        "# Function to download and extract the dataset\n",
        "def download_and_extract_dataset(dataset):\n",
        "    if DATA_SOURCE == \"Kaggle\":\n",
        "\n",
        "        if NOTEBOOK == \"Colab\":\n",
        "            # Mount Google Drive if not already mounted\n",
        "            from google.colab import drive\n",
        "            drive_mounted = os.path.exists(DRIVE_PATH)\n",
        "            if not drive_mounted:\n",
        "                drive.mount(DRIVE_PATH)\n",
        "\n",
        "        load_dotenv(ENV_PATH)  # Load Kaggle credentials from .env\n",
        "\n",
        "        print(\"\\nINFO: Kaggle API Authentication successful.\")\n",
        "\n",
        "        zip_file_path = f\"{dataset.split('/')[-1]}.zip\"\n",
        "\n",
        "        if not os.path.exists(zip_file_path):\n",
        "            os.system(f\"kaggle datasets download -d {dataset}\")\n",
        "\n",
        "        print(\"INFO: Downloading finished.\")\n",
        "\n",
        "        # Define the extraction directory based on the notebook environment\n",
        "        if NOTEBOOK == \"Kaggle\":\n",
        "            extracted_dir = \"/kaggle/input/\" + DATASET_NAME\n",
        "        if NOTEBOOK == \"Colab\":\n",
        "            extracted_dir = os.path.abspath(PROJECT_NAME)\n",
        "\n",
        "        # Create the extraction directory if it doesn't exist\n",
        "        if not os.path.exists(extracted_dir):\n",
        "            os.makedirs(extracted_dir)\n",
        "\n",
        "        # Extract the contents of the zip file\n",
        "        if not os.listdir(extracted_dir):\n",
        "            with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(extracted_dir)\n",
        "            print(\"INFO: Extraction completed.\")\n",
        "        else:\n",
        "            print(\"INFO: Extraction already completed. Skipping...\")\n",
        "\n",
        "        print(f\"INFO: Contents of {zip_file_path} extracted to \\\"{extracted_dir}\\\".\\n\")\n",
        "\n",
        "        if NOTEBOOK == \"Colab\":\n",
        "            if not os.path.exists(COLAB_WORKING_PATH):\n",
        "                os.makedirs(COLAB_WORKING_PATH)\n",
        "\n",
        "            train_path = os.path.join(COLAB_WORKING_PATH, DATASET_DIR, TRAIN_DIR)\n",
        "            test_path = os.path.join(COLAB_WORKING_PATH, DATASET_DIR, TEST_DIR)\n",
        "\n",
        "            print(f\"Train Path: {train_path}\")\n",
        "            print(f\"Test Path: {test_path}\")\n",
        "\n",
        "            return train_path, test_path\n",
        "\n",
        "        if NOTEBOOK == \"Kaggle\":\n",
        "            if not os.path.exists(KAGGLE_WORKING_PATH):\n",
        "                os.makedirs(KAGGLE_WORKING_PATH)\n",
        "\n",
        "            train_path = os.path.join(KAGGLE_WORKING_PATH, DATASET_DIR, TRAIN_DIR)\n",
        "            test_path = os.path.join(KAGGLE_WORKING_PATH, DATASET_DIR, TEST_DIR)\n",
        "\n",
        "            print(f\"Train Path: {train_path}\")\n",
        "            print(f\"Test Path: {test_path}\")\n",
        "\n",
        "            return train_path, test_path\n",
        "\n",
        "\n",
        "# Call the download_and_extract_dataset function\n",
        "TRAIN_PATH, TEST_PATH = download_and_extract_dataset(os.path.join(USERNAME, DATASET_NAME))\n",
        "\n",
        "# Define input shape\n",
        "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
        "print(f\"Input Shape: {INPUT_SHAPE}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K79teZGg5GgG"
      },
      "outputs": [],
      "source": [
        "# @title # Load the Dataset Optimized { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "# @markdown # Configuration:\n",
        "VALIDATION_SPLIT = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "NUM_CLASSES = 131 # @param {type:\"integer\"}\n",
        "\n",
        "# Function to load and decode an image\n",
        "def load_and_decode_image(label, directory, image_file):\n",
        "    file_path = os.path.join(directory, image_file)\n",
        "    with tf.io.gfile.GFile(file_path, \"rb\") as f:\n",
        "        image_data = f.read()\n",
        "        image = tf.image.decode_jpeg(image_data, channels=CHANNELS)\n",
        "    return image, label\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images_and_labels(parent_directory, executor):\n",
        "    subdirectories = [d for d in os.listdir(parent_directory) if os.path.isdir(os.path.join(parent_directory, d))]\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for label, subdirectory in enumerate(subdirectories):\n",
        "        directory = os.path.join(parent_directory, subdirectory)\n",
        "        image_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "        # Use executor.map to parallelize loading and decoding\n",
        "        results = list(executor.map(load_and_decode_image, [label] * len(image_files), [directory] * len(image_files), image_files))\n",
        "        image_batch, label_batch = zip(*results)\n",
        "        images.extend(image_batch)\n",
        "        labels.extend([label] * len(image_files))\n",
        "\n",
        "    labels = keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)  # One-hot encode the labels\n",
        "    return images, labels\n",
        "\n",
        "# Function to load datasets\n",
        "def load_dataset(validation_split=None):\n",
        "    validation_split = None if validation_split == 0 else validation_split\n",
        "\n",
        "    if ACCELERATOR == \"TPU\":\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            train_images, train_labels = load_images_and_labels(TRAIN_PATH, executor)\n",
        "            test_images, test_labels = load_images_and_labels(TEST_PATH, executor)\n",
        "\n",
        "        num_validation_samples = int(len(train_images) * validation_split) if validation_split else 0\n",
        "\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "        train_ds = train_ds.shuffle(buffer_size=len(train_ds), seed=1).skip(num_validation_samples)\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(buffer_size=len(train_ds))\n",
        "\n",
        "        if validation_split:\n",
        "            val_ds = train_ds.shuffle(buffer_size=len(train_ds), seed=1).take(num_validation_samples)\n",
        "            return train_ds, val_ds, test_ds\n",
        "        else:\n",
        "            return train_ds, test_ds\n",
        "\n",
        "    if ACCELERATOR == \"GPU\":\n",
        "        def load_dataset(path, subset=None):\n",
        "            subset = None if validation_split is None else subset\n",
        "\n",
        "            return keras.utils.image_dataset_from_directory(\n",
        "                path,\n",
        "                labels=\"inferred\",\n",
        "                label_mode=\"categorical\",\n",
        "                color_mode=\"rgba\" if CHANNELS == 4 else \"grayscale\" if CHANNELS == 1 else \"rgb\",\n",
        "                batch_size=None,\n",
        "                image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                shuffle=True,\n",
        "                seed=1,\n",
        "                validation_split=validation_split,\n",
        "                subset=subset\n",
        "            )\n",
        "\n",
        "        train_ds = load_dataset(TRAIN_PATH, subset=\"training\", validation_split=validation_split)\n",
        "        test_ds = load_dataset(TEST_PATH)\n",
        "\n",
        "        if validation_split:\n",
        "            val_ds = load_dataset(TRAIN_PATH, subset=\"validation\", validation_split=validation_split)\n",
        "            return train_ds, val_ds, test_ds\n",
        "        else:\n",
        "            return train_ds, test_ds\n",
        "\n",
        "# Load the datasets\n",
        "if VALIDATION_SPLIT != 0:\n",
        "    train_ds, val_ds, test_ds = load_dataset(validation_split=VALIDATION_SPLIT)\n",
        "    print(f\"================== INFO ==================\")\n",
        "    print(f\"Length of the Training Dataset: {len(train_ds)}\")\n",
        "    print(f\"Length of the Validation Dataset: {len(val_ds)}\")\n",
        "    print(f\"Length of the Testing Dataset: {len(test_ds)}\")\n",
        "else:\n",
        "    train_ds, test_ds = load_dataset()\n",
        "    print(f\"================== INFO ==================\")\n",
        "    print(f\"Length of the Training Dataset: {len(train_ds)}\")\n",
        "    print(f\"Length of the Testing Dataset: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGCPgcT39Y_O"
      },
      "outputs": [],
      "source": [
        "# @title # Explatory Data Analysis { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "NUM_ROWS = 5 # @param {type:\"integer\"}\n",
        "NUM_COLUMNS = 8 # @param {type:\"integer\"}\n",
        "NUM_SAMPLES = 40 # @param {type:\"integer\"}\n",
        "\n",
        "# Function for EDA\n",
        "def exploratory_data_analysis(num_samples, num_rows, num_cols):\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    plt.suptitle(\"Exploratory Data Analysis\", fontsize=24)\n",
        "\n",
        "    if ACCELERATOR == \"TPU\":\n",
        "        parent_directory = os.path.join(COLAB_WORKING_PATH, DATASET_DIR, TRAIN_DIR)\n",
        "        subdirectories = [d for d in os.listdir(parent_directory) if os.path.isdir(os.path.join(parent_directory, d))]\n",
        "        random_subdirectories = random.sample(subdirectories, num_samples)\n",
        "        images = []\n",
        "\n",
        "        for random_subdirectory in random_subdirectories:\n",
        "            directory = os.path.join(parent_directory, random_subdirectory)\n",
        "            image_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "            random_image = random.choice(image_files)\n",
        "            file_path = os.path.join(directory, random_image)\n",
        "\n",
        "            with tf.io.gfile.GFile(file_path, \"rb\") as f:\n",
        "                image_data = f.read()\n",
        "                image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "                images.append(image)\n",
        "\n",
        "        for i, image in enumerate(images):\n",
        "            plt.subplot(num_rows, num_cols, i+1)\n",
        "            plt.imshow(image.numpy().astype(\"float32\") / 255.0)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "    elif ACCELERATOR == \"GPU\":\n",
        "        for i, (x, y) in enumerate(train_ds.take(num_samples)):\n",
        "            image = x\n",
        "            plt.subplot(num_rows, num_cols, i+1)\n",
        "            plt.imshow(image.numpy().astype(\"float32\") / 255.0)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Perform EDA\n",
        "exploratory_data_analysis(NUM_SAMPLES, NUM_ROWS, NUM_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgBCYSld9Y_Q"
      },
      "outputs": [],
      "source": [
        "# @title # Data Augmentation { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "NUM_ROWS = 5 # @param {type:\"integer\"}\n",
        "NUM_COLUMNS = 8 # @param {type:\"integer\"}\n",
        "NUM_SAMPLES = 40 # @param {type:\"integer\"}\n",
        "AUGMENTATIONS_PER_IMAGE = 1 # @param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
        "MAGNITUDE = 0.33 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "MAGNITUDE_STDDEV = 0.1 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "def plot_augmentation(augmentation_fn, num_samples, num_rows, num_cols):\n",
        "    for x, y in train_ds.batch(1):\n",
        "        image = x[0].numpy()\n",
        "        break\n",
        "\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(num_rows, num_cols, i+1)\n",
        "        plt.imshow(augmentation_fn(image) / 255.0)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "def augment_image(image, label):\n",
        "    image = rand_augment(image)\n",
        "    return image, label\n",
        "\n",
        "# Random Augmentation Layer\n",
        "rand_augment = keras_cv.layers.RandAugment(\n",
        "    value_range=[0, 255],\n",
        "    augmentations_per_image=AUGMENTATIONS_PER_IMAGE,\n",
        "    magnitude=MAGNITUDE,\n",
        "    magnitude_stddev=MAGNITUDE_STDDEV\n",
        ")\n",
        "\n",
        "plot_augmentation(rand_augment, NUM_SAMPLES, NUM_ROWS, NUM_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyyCbVE19Y_O"
      },
      "outputs": [],
      "source": [
        "# @title # Create Data Pipelines { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "BATCH_SIZE = 128 # @param {type:\"integer\"}\n",
        "\n",
        "STEPS_PER_EPOCH = len(train_ds) // BATCH_SIZE\n",
        "VAL_STEPS = len(val_ds) // BATCH_SIZE\n",
        "\n",
        "def create_data_pipeline(dataset, repeat=False, augment=False):\n",
        "    if augment:\n",
        "        return (\n",
        "            dataset\n",
        "            .shuffle(buffer_size=len(dataset))\n",
        "            .map(augment_image, num_parallel_calls=AUTO)\n",
        "            .repeat()\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(AUTO)\n",
        "        )\n",
        "    elif repeat:\n",
        "        return (\n",
        "            dataset\n",
        "            .shuffle(buffer_size=len(dataset))\n",
        "            .repeat()\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(AUTO)\n",
        "        )\n",
        "    else:\n",
        "        return (\n",
        "            dataset\n",
        "            .shuffle(buffer_size=len(dataset))\n",
        "            .batch(BATCH_SIZE)\n",
        "            .prefetch(AUTO)\n",
        "        )\n",
        "\n",
        "# Create data pipelines\n",
        "val_ds = create_data_pipeline(val_ds)\n",
        "test_ds = create_data_pipeline(test_ds)\n",
        "\n",
        "print(f\"INFO: Datasets have been perfectly shuffled, batched in size of {BATCH_SIZE} and prefetched.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yIA8pzb9Y_R",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title # Base VGG16 Model { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "def basic_vgg16(include_top=False, weights=\"imagenet\", input_shape: tuple=INPUT_SHAPE):\n",
        "    # Get VGG16 model with ImageNet weights\n",
        "    basic_vgg16 = keras.applications.vgg16.VGG16(\n",
        "        include_top=include_top,\n",
        "        weights=weights,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    return basic_vgg16\n",
        "\n",
        "basic_vgg16 = basic_vgg16()\n",
        "basic_vgg16.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-D_COP89Y_S"
      },
      "outputs": [],
      "source": [
        "# @title # Customized VGG16 Model { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "LOSS = \"categorical_crossentropy\" # @param {type:\"raw\"}\n",
        "METRICS = [\"accuracy\"] # @param {type:\"raw\"}\n",
        "REGULARIZER = None # @param {type:\"raw\"}\n",
        "DROPOUT_RATE = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "def Fruits360_VGG16(input_shape, num_classes, regularizer=None, dropout_rate=0.5, weights=\"imagenet\"):\n",
        "    # Load pre-trained VGG16 model\n",
        "    vgg16 = keras.applications.vgg16.VGG16(\n",
        "        include_top=False,\n",
        "        weights=weights,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Define the input layer\n",
        "    input_layer = keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Preprocess input\n",
        "    x = keras.applications.vgg16.preprocess_input(input_layer)\n",
        "\n",
        "    x = keras.layers.Rescaling(1./255)(x)\n",
        "\n",
        "    # Pass through VGG16 layers\n",
        "    x = vgg16(x)\n",
        "\n",
        "    # Flatten and add fully connected layers\n",
        "    x = keras.layers.Flatten(name=\"flattening_layer\")(x)\n",
        "    x = keras.layers.Dropout(rate=dropout_rate)(x)\n",
        "    x = keras.layers.Dense(units=4096, activation=\"relu\", kernel_regularizer=REGULARIZER, name=\"fc1\")(x)\n",
        "    x = keras.layers.Dense(units=4096, activation=\"relu\", kernel_regularizer=REGULARIZER, name=\"fc2\")(x)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = keras.layers.Dense(units=num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer, name=PROJECT_NAME)\n",
        "\n",
        "    # Freeze VGG16 layers\n",
        "    for layer in model.layers:\n",
        "        if layer.name == \"vgg16\":\n",
        "            layer.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_and_compile_model(optimizer, loss, metrics):\n",
        "    if ACCELERATOR == \"TPU\":\n",
        "        with TPU_STRATEGY.scope():\n",
        "            model = Fruits360_VGG16(INPUT_SHAPE, NUM_CLASSES, regularizer=REGULARIZER, dropout_rate=DROPOUT_RATE)\n",
        "            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    elif ACCELERATOR == \"GPU\":\n",
        "        model = Fruits360_VGG16(INPUT_SHAPE, NUM_CLASSES, regularizer=REGULARIZER, dropout_rate=DROPOUT_RATE)\n",
        "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_and_compile_model(keras.optimizers.SGD(learning_rate=0.01, momentum=0.96, nesterov=True, weight_decay=1e-6), LOSS, METRICS)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hke50yfp9Y_S"
      },
      "outputs": [],
      "source": [
        "# @title # Define Callbacks and Functions { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 3 # @param {type:\"slider\", min:0, max:20, step:1}\n",
        "REDUCE_LR_PATIENCE = 2 # @param {type:\"slider\", min:0, max:20, step:1}\n",
        "WARMUP_EPOCHS = 5 # @param {type:\"slider\", min:0, max:10, step:1}\n",
        "INITIAL_LR = 0.0001 # @param {type:\"slider\", min:0.0001, max:0.001, step:0.0001}\n",
        "TARGET_LR = 0.01 # @param {type:\"slider\", min:0.01, max:0.1, step:0.01}\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "LOGS_DIR = None\n",
        "CHECKPOINT_OPTIONS = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "def define_callbacks(checkpoint_options):\n",
        "    global LOGS_DIR\n",
        "    # Define the timezone you want to use (GMT+3)\n",
        "    timezone = pytz.timezone(\"EET\")\n",
        "    current_time = datetime.datetime.now(timezone)\n",
        "    timestamp = current_time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "\n",
        "    # Define log directory with timestamp\n",
        "    LOGS_DIR = os.path.join(PROJECT_NAME, \"logs\", timestamp)\n",
        "    saved_model_dir = os.path.join(LOGS_DIR, \"saved_model\")\n",
        "    checkpoint_file = os.path.join(saved_model_dir, \"checkpoint\")\n",
        "\n",
        "    # Define callbacks\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(filepath=checkpoint_file, options=checkpoint_options, save_best_only=True, save_weights_only=True, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
        "    ]\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "\n",
        "def save_history(history_list, file_path):\n",
        "    results = []\n",
        "\n",
        "    # Iterate through each History object\n",
        "    for history in history_list:\n",
        "        history_dict = {}\n",
        "\n",
        "        # Convert float32 values to float and lists to lists of floats\n",
        "        for key in history.history.keys():\n",
        "            history_dict[key] = [float(val) for val in history.history[key]]\n",
        "\n",
        "        results.append(history_dict)\n",
        "\n",
        "    # Save history to a JSON file\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(results, json_file)\n",
        "\n",
        "\n",
        "def load_history(file_path):\n",
        "    # Load history from a JSON file\n",
        "    with open(file_path, \"r\") as f:\n",
        "        loaded_history = json.load(f)\n",
        "\n",
        "    return loaded_history\n",
        "\n",
        "\n",
        "def LearningRateWarmupCallback(epoch, model):\n",
        "    def warmup_learning_rate(epoch, warmup_epochs, initial_learning_rate, target_learning_rate):\n",
        "        # Calculate the current learning rate based on the warm-up schedule\n",
        "        if epoch < warmup_epochs:\n",
        "            alpha = epoch / warmup_epochs\n",
        "            current_learning_rate = initial_learning_rate + alpha * (target_learning_rate - initial_learning_rate)\n",
        "        else:\n",
        "            current_learning_rate = target_learning_rate\n",
        "\n",
        "        return current_learning_rate\n",
        "\n",
        "    if epoch < WARMUP_EPOCHS:\n",
        "        # Warmup function\n",
        "        lr = warmup_learning_rate(epoch, WARMUP_EPOCHS, INITIAL_LR, TARGET_LR)\n",
        "        # Assing the learning rate\n",
        "        model.optimizer.lr.assign(lr)\n",
        "\n",
        "        print(f\"INFO: Warmup Round: {epoch + 1}/{WARMUP_EPOCHS}, Learning Rate: {model.optimizer.lr.numpy():.2e}\")\n",
        "    elif epoch == WARMUP_EPOCHS:\n",
        "        # Warmup function\n",
        "        lr = warmup_learning_rate(epoch, WARMUP_EPOCHS, INITIAL_LR, TARGET_LR)\n",
        "        # Assing the learning rate\n",
        "        model.optimizer.lr.assign(lr)\n",
        "\n",
        "        print(f\"INFO: Warmup rounds finished. Learning Rate: {model.optimizer.lr.numpy():.2e}\")\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "def ReduceLRCallback(epoch, model, patience, monitor=\"val_loss\", factor=0.1, min_lr=0, verbose=0):\n",
        "    global cooldown, best_monitor_value_1  # Use the global cooldown variable\n",
        "\n",
        "    if epoch >= WARMUP_EPOCHS:\n",
        "        current_lr = model.optimizer.lr.numpy()\n",
        "        monitor_value = model.history.history[monitor][-1]\n",
        "\n",
        "        if monitor_value < best_monitor_value_1:\n",
        "            cooldown = 0\n",
        "            best_monitor_value_1 = monitor_value\n",
        "        else:\n",
        "            cooldown += 1\n",
        "\n",
        "        if cooldown >= patience:\n",
        "            new_lr = max(current_lr * factor, min_lr)\n",
        "            model.optimizer.lr.assign(new_lr)  # Update the learning rate\n",
        "            cooldown = 0  # Reset cooldown\n",
        "            if verbose > 0:\n",
        "                print(f\"\\nINFO: Learning rate reduced to {new_lr:.2e}\\n\")\n",
        "        else:\n",
        "            print(\"INFO: Learning rate not reduced.\")\n",
        "            print(f\"Current LR: {current_lr:.2e}\")\n",
        "\n",
        "\n",
        "def EarlyStoppingCallback(epoch, model, patience, monitor=\"val_loss\", verbose=0):\n",
        "    global early_stopping_counter, best_monitor_value_2\n",
        "\n",
        "    if epoch >= WARMUP_EPOCHS:\n",
        "        monitor_value = model.history.history[monitor][-1]\n",
        "\n",
        "        if monitor_value < best_monitor_value_2:\n",
        "            early_stopping_counter = 0\n",
        "            best_monitor_value_2 = monitor_value\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            early_stopping_counter = 0\n",
        "            if verbose > 0:\n",
        "                print(f\"\\nINFO: Early stopping triggered at epoch {epoch + 1}.\\nBest {monitor} so far: {best_monitor_value_2:.4e}.\\nCheckpoint saved and ready to load.\\n\")\n",
        "            return True  # Return True to signal early stopping\n",
        "        else:\n",
        "            print(\"INFO: Early stopping not triggered.\")\n",
        "            print(f\"Current Best {monitor}: {best_monitor_value_2:.4e}\")\n",
        "\n",
        "    return False  # Return False to continue training\n",
        "\n",
        "\n",
        "callbacks = define_callbacks(CHECKPOINT_OPTIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKDoQAiQ9Y_T",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title # Model Training Loop { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "EPOCHS = 50 # @param {type:\"slider\", min:0, max:100, step:1}\n",
        "TRAIN_COUNT = 4 # @param {type:\"slider\", min:0, max:20, step:1}\n",
        "\n",
        "cooldown, early_stopping_counter = 0, 0\n",
        "best_monitor_value_1, best_monitor_value_2 = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "# Training loop\n",
        "def training_loop(train_ds, val_ds, augmentation_fn, initial_weights):\n",
        "    global cooldown, early_stopping_counter, best_monitor_value_1, best_monitor_value_2\n",
        "    results = []\n",
        "    model = None\n",
        "\n",
        "    for i in range(TRAIN_COUNT):\n",
        "        print(f\"[===================================== LOOP {i+1} =====================================]\\n\")\n",
        "\n",
        "        cooldown, early_stopping_counter = 0, 0\n",
        "        best_monitor_value_1, best_monitor_value_2 = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "        # Create and compile the model\n",
        "        model = create_and_compile_model(keras.optimizers.SGD(learning_rate=0.01, momentum=0.96, nesterov=True, weight_decay=1e-6), LOSS, METRICS)\n",
        "\n",
        "        # Reinitialize model weights for each loop\n",
        "        model.set_weights(initial_weights)\n",
        "\n",
        "        # Get callbacks\n",
        "        callbacks = define_callbacks(CHECKPOINT_OPTIONS)\n",
        "\n",
        "        loop_history = []  # List to store history for this loop\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            # Apply learning rate warmup\n",
        "            LearningRateWarmupCallback(epoch, model)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{EPOCHS}:\")\n",
        "\n",
        "            # Augment the dataset for this epoch\n",
        "            augmented_train_ds = create_data_pipeline(train_ds, augment=True)\n",
        "\n",
        "            # Train the model for one epoch\n",
        "            history = model.fit(\n",
        "                augmented_train_ds,\n",
        "                epochs=1,\n",
        "                validation_data=val_ds,\n",
        "                steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                validation_steps=VAL_STEPS,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "            # Reduce learning rate if monitored metric stopped improvement\n",
        "            ReduceLRCallback(epoch, model, REDUCE_LR_PATIENCE, verbose=1)\n",
        "\n",
        "            # Check for early stopping\n",
        "            if EarlyStoppingCallback(epoch, model, EARLY_STOPPING_PATIENCE, verbose=1):\n",
        "                break\n",
        "\n",
        "            print()\n",
        "\n",
        "            # Append this epoch's history to the loop_history list\n",
        "            loop_history.append(history)\n",
        "\n",
        "        # Save training history for this loop\n",
        "        saved_model_dir = os.path.join(LOGS_DIR, \"saved_model\")\n",
        "        json_file_path = os.path.join(saved_model_dir, \"history.json\")\n",
        "        save_history(loop_history, json_file_path)\n",
        "\n",
        "        # Collect metrics for this training run\n",
        "        train_accuracy = loop_history[-1].history[\"accuracy\"][-1]\n",
        "        train_loss = loop_history[-1].history[\"loss\"][-1]\n",
        "        val_accuracy = loop_history[-1].history[\"val_accuracy\"][-1]\n",
        "        val_loss = loop_history[-1].history[\"val_loss\"][-1]\n",
        "\n",
        "        results.append({\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_accuracy\": val_accuracy,\n",
        "            \"val_loss\": val_loss,\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Initialize weights before training loop\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Run training loop and get results\n",
        "results = training_loop(train_ds, val_ds, augment_image, initial_weights)\n",
        "\n",
        "# Calculate and print average metrics\n",
        "average_train_accuracy = np.mean([result[\"train_accuracy\"] for result in results])\n",
        "average_train_loss = np.mean([result[\"train_loss\"] for result in results])\n",
        "average_val_accuracy = np.mean([result[\"val_accuracy\"] for result in results])\n",
        "average_val_loss = np.mean([result[\"val_loss\"] for result in results])\n",
        "\n",
        "print()\n",
        "print(f\"Average Train Accuracy: %{100 * average_train_accuracy:.2f}\")\n",
        "print(f\"Average Train Loss: {average_train_loss:.2f}\")\n",
        "print(f\"Average Validation Accuracy: %{100 * average_val_accuracy:.2f}\")\n",
        "print(f\"Average Validation Loss: {average_val_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Load and Evaluate the Best Model { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "def get_best_model_dir(project_name):\n",
        "    min_val_loss = float(\"inf\")\n",
        "    best_timestamp = None\n",
        "\n",
        "    for log in os.listdir(os.path.join(project_name, \"logs\")):\n",
        "        log_dir = os.path.join(project_name, \"logs\", log)\n",
        "\n",
        "        if os.path.isdir(log_dir):\n",
        "            saved_model_dir = os.path.join(log_dir, \"saved_model\")\n",
        "\n",
        "            if os.path.isdir(saved_model_dir):\n",
        "                history_file = os.path.join(saved_model_dir, \"history.json\")\n",
        "\n",
        "                if os.path.isfile(history_file):\n",
        "                    history = load_history(history_file)\n",
        "                    for entry in history:\n",
        "                        min_val_loss_current = min(entry[\"val_loss\"])\n",
        "                        if min_val_loss_current < min_val_loss:\n",
        "                            min_val_loss = min_val_loss_current\n",
        "                            best_timestamp = log\n",
        "\n",
        "    best_model_dir = os.path.join(project_name, \"logs\", best_timestamp)\n",
        "    return best_model_dir\n",
        "\n",
        "def load_best_model(strategy, checkpoint_file, checkpoint_options):\n",
        "    with strategy.scope():\n",
        "        model = create_and_compile_model(keras.optimizers.SGD(learning_rate=0.01, momentum=0.96, nesterov=True, weight_decay=1e-6), LOSS, METRICS)\n",
        "        load_status = model.load_weights(checkpoint_file, options=checkpoint_options)\n",
        "\n",
        "        if load_status.assert_existing_objects_matched():\n",
        "            print(\"Model weights have been successfully loaded.\\n\")\n",
        "        else:\n",
        "            print(\"Failed to load model weights.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, train_dataset, test_dataset, steps_per_epoch):\n",
        "    # Evaluate the model\n",
        "    train_dataset = create_data_pipeline(train_dataset, repeat=True)\n",
        "    print(\"Train Evaluation:\")\n",
        "    train_eval = model.evaluate(train_dataset, steps=STEPS_PER_EPOCH, verbose=1)\n",
        "    train_loss, train_acc = train_eval\n",
        "    print(\"\\nTest Evaluation:\")\n",
        "    test_eval = model.evaluate(test_ds, verbose=1)\n",
        "    test_loss, test_acc = test_eval\n",
        "    # Print evaluation results\n",
        "    print(f\"\\nTrain Loss: {train_loss:.4f}\", f\"Train Accuracy: %{100 * (train_acc):.2f}\", sep=\"\\n\")\n",
        "    print(f\"Test Loss:  {test_loss:.4f}\" , f\"Test Accuracy:  %{100 *(test_acc):.2f}\", sep=\"\\n\")\n",
        "\n",
        "    return train_eval, test_eval\n",
        "\n",
        "\n",
        "best_model_dir = get_best_model_dir(PROJECT_NAME)\n",
        "best_model_checkpoint_file = os.path.join(best_model_dir, \"saved_model\", \"checkpoint\")\n",
        "best_model_history_file = os.path.join(best_model_dir, \"saved_model\", \"history.json\")\n",
        "\n",
        "# Load the best model\n",
        "best_model = load_best_model(TPU_STRATEGY, best_model_checkpoint_file, CHECKPOINT_OPTIONS)\n",
        "\n",
        "# Load the best history\n",
        "best_history = load_history(best_model_history_file)\n",
        "\n",
        "# Evaluate the best model\n",
        "train_eval, test_eval = evaluate_model(best_model, train_ds, test_ds, STEPS_PER_EPOCH)"
      ],
      "metadata": {
        "id": "yDvofBGLQM7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVMFqhMg9Y_T"
      },
      "outputs": [],
      "source": [
        "# @title # Training and Validation Graph { vertical-output: true, form-width: \"33%\", display-mode: \"form\" }\n",
        "\n",
        "# Function to concatenate history dictionaries\n",
        "def concatenate_history(history_list):\n",
        "    concatenated_history = {}\n",
        "\n",
        "    for history_dict in history_list:\n",
        "        for key, value in history_dict.items():\n",
        "            if key not in concatenated_history:\n",
        "                concatenated_history[key] = value\n",
        "            else:\n",
        "                concatenated_history[key].extend(value)\n",
        "\n",
        "    return concatenated_history\n",
        "\n",
        "# Function to plot metrics\n",
        "def plot_metrics(history, title):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Loss plot\n",
        "    axes[0].plot(history[\"loss\"], label=\"Training Loss\")\n",
        "    axes[0].plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axes[0].set_xlabel(\"Epochs\", fontsize=12)\n",
        "    axes[0].set_ylabel(\"Loss\", fontsize=12)\n",
        "    axes[0].set_title(\"Loss per Epoch\", fontsize=14)\n",
        "    axes[0].legend(fontsize=12)\n",
        "    axes[0].set_xticks(np.arange(0, len(history[\"loss\"]), 5))\n",
        "    axes[0].set_xticklabels(np.arange(0, len(history[\"loss\"]), 5))\n",
        "    axes[0].set_xlim(0, len(history[\"loss\"]))\n",
        "    axes[0].set_ylim(0, 50)\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[1].plot(np.array(history[\"accuracy\"]) * 100, label=\"Training Accuracy\")\n",
        "    axes[1].plot(np.array(history[\"val_accuracy\"]) * 100, label=\"Validation Accuracy\")\n",
        "    axes[1].set_xlabel(\"Epochs\", fontsize=12)\n",
        "    axes[1].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "    axes[1].set_title(\"Accuracy per Epoch\", fontsize=14)\n",
        "    axes[1].legend(fontsize=12)\n",
        "    axes[1].set_xticks(np.arange(0, len(history[\"accuracy\"]), 5))\n",
        "    axes[1].set_xticklabels(np.arange(0, len(history[\"accuracy\"]), 5))\n",
        "    axes[1].set_xlim(0, len(history[\"accuracy\"]))\n",
        "    axes[1].set_ylim(50, 100)\n",
        "\n",
        "    fig.suptitle(title, fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Concatenate history dictionaries\n",
        "concatenated_history = concatenate_history(best_history)\n",
        "\n",
        "# Modify loaded_history to your loaded history data\n",
        "plot_metrics(concatenated_history, \"Training and Validation Metrics\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Reset Logs\n",
        "RESET = False # @param {type:\"boolean\"}\n",
        "\n",
        "if RESET:\n",
        "    !rm -r Fruits360_VGG16/logs\n",
        "    print(\"Logs resetted.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yqok3K3idAHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}